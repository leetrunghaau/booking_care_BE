const createError = require("http-errors");
const aiConfig = require("../../../config/ai-config");

class AI {
    static async chat(req, res, next) {
        try {
            let message = req.body?.message;

            // Step 1: G·ªçi ƒë·∫øn RAG API ƒë·ªÉ l·∫•y t√†i li·ªáu li√™n quan
            const ragResponse = await fetch(`${aiConfig.search_URL}/search`, {
                method: "POST",
                headers: { "Content-Type": "application/json" },
                body: JSON.stringify({ query: message }),
            });

            const ragData = await ragResponse.json();
            let contextText = "";

            if (ragData.found && Array.isArray(ragData.results)) {
                // N·ªëi c√°c t√†i li·ªáu t√¨m ƒë∆∞·ª£c th√†nh 1 context
                contextText = ragData.results.map(r => r.text).join("\n---\n");
                console.log("üìö RAG t√¨m th·∫•y context:", contextText.slice(0, 200), "...");
            } else {
                console.log("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y t√†i li·ªáu t·ª´ RAG. G·ª≠i message thu·∫ßn.");
            }

            // Step 2: G·ª≠i t·ªõi Ollama k√®m context (n·∫øu c√≥)
            const ollamaRequestBody = {
                model: "medplus",
                stream: true,
                messages: [],
                
            };

            if (contextText) {
                // Ch√®n context tr·ª±c ti·∫øp v√†o c√¢u h·ªèi ng∆∞·ªùi d√πng
                message = `D∆∞·ªõi ƒë√¢y l√† t√†i li·ªáu tham kh·∫£o:\n${contextText}\n\nVui l√≤ng tr·∫£ l·ªùi th·∫≠t ng·∫Øn g·ªçn, t·ªëi ƒëa 3 c√¢u. C√¢u h·ªèi c·ªßa t√¥i:  ${message}`;
            }

            ollamaRequestBody.messages.push({
                role: "user",
                content: message
            });

            const response = await fetch(`${aiConfig.medly_URL}/api/chat`, {
                method: "POST",
                headers: { "Content-Type": "application/json" },
                body: JSON.stringify(ollamaRequestBody),
            });

            // Step 3: Stream response t·ª´ Ollama v·ªÅ frontend
            res.setHeader("Content-Type", "text/event-stream");
            res.setHeader("Cache-Control", "no-cache");
            res.setHeader("Connection", "keep-alive");

            const reader = response.body.getReader();
            const decoder = new TextDecoder();

            while (true) {
                const { done, value } = await reader.read();
                if (done) {
                    console.log("‚úÖ K·∫øt th√∫c ƒë·ªçc stream t·ª´ AI.");
                    res.end();
                    break;
                }
                const chunk = decoder.decode(value, { stream: true });

                try {
                    const chunkOBJ = JSON.parse(chunk);
                    res.write(chunkOBJ.message?.content || "");
                } catch (e) {
                    console.error("‚ùå L·ªói parse chunk:", chunk);
                }
            }
        } catch (error) {
            console.error("‚ùå L·ªói t·ªïng th·ªÉ:", error);
            return next(createError.InternalServerError());
        }
    }
}

module.exports = AI;
